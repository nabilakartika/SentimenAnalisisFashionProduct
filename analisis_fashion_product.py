# -*- coding: utf-8 -*-
"""NLP (Nabilah)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zOqao0YsWmFwdDrm2s6jslCvKE_NLVMM
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("nicapotato/womens-ecommerce-clothing-reviews")

print("Path to dataset files:", path)

import os

files = os.listdir(path)
print("Files in dataset directory:", files)

import pandas as pd

file_path = os.path.join(path, 'Womens Clothing E-Commerce Reviews.csv')
data = pd.read_csv(file_path)

print(data.head())

missing_values = data.isnull().sum()
print("Missing values in each column:")
print(missing_values)

data = data.dropna()
print("After dropping missing values:")
print(data.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.countplot(x='Recommended IND', data=data)
plt.title('Distribution of Recomended')
plt.xlabel('Recomend')
plt.ylabel('Count')
plt.show()

import re

def clean_text(text):
    text = text.lower() #lower text
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #remove character
    text = re.sub(r'\s+', ' ', text) #single space
    words = text.split()  # Split text into a list of words
    unique_words = []     # Initialize a list to hold unique words
    for word in words:
        if word not in unique_words:
            unique_words.append(word)  # Add word if not already in the list
    # Rejoin unique words into a cleaned string
    return ' '.join(unique_words)

data['cleaned_reviews'] = data['Review Text'].fillna("").apply(clean_text)
data[['Review Text', 'cleaned_reviews']]

data["cleaned_reviews"].iloc[0]

data["cleaned_reviews"].iloc[7]

"""# SPACY"""

import spacy

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

data['spacy_cleaned'] = data['cleaned_reviews'].apply(lambda x: nlp(x))
data[['cleaned_reviews', 'spacy_cleaned']]

# Extract nouns and adjectives from each review
def extract_keywords(doc):
    # Extract nouns and adjectives
    keywords = [token.text for token in doc if token.pos_ in ["NOUN", "ADJ"]]
    return ' '.join(keywords)

# Apply extraction to the spacy_cleaned column
data['keywords'] = data['spacy_cleaned'].apply(lambda doc: extract_keywords(doc))
data['keywords']

# Extract lemmatized text
def lemmatize_text(doc):
    lemmatized = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]
    return ' '.join(lemmatized)

data['lemmatized'] = data['spacy_cleaned'].apply(lambda doc: lemmatize_text(doc))
data['lemmatized']

data['keywords'].iloc[0]

data['lemmatized'].iloc[0]

from collections import Counter

# Combine all keywords for frequency analysis
all_keywords = ' '.join(data['keywords'])
keyword_list = all_keywords.split()

# Count keyword frequencies
keyword_freq = Counter(keyword_list)
print(keyword_freq.most_common(10))  # Display top 20 keywords

def count_keywords(row):
    keyword_list = row.split()  # Split keywords in the row
    keyword_freq = Counter(keyword_list)  # Count frequencies
    return dict(keyword_freq)  # Return as a dictionary for each row

data['clear'] = data['keywords'].apply(count_keywords)
data['clear']

data['clear'].iloc[0]

data['clear'].iloc[7]

from textblob import TextBlob
import pandas as pd

# Fungsi untuk analisis sentimen
def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity  # Polarity dari -1 hingga 1

# Tambahkan kolom sentimen
data['sentiment_polarity'] = data['keywords'].apply(analyze_sentiment)

# Fungsi untuk konversi polaritas ke skala 1-5
def convert_to_rating(polarity):
    if polarity <= -0.6:
        return 1  # Sangat negatif
    elif polarity <= -0.2:
        return 2  # Negatif
    elif polarity <= 0.2:
        return 3  # Netral
    elif polarity <= 0.6:
        return 4  # Positif
    else:
        return 5  # Sangat positif

# Tambahkan kolom rating berdasarkan sentimen
data['sentiment_rating'] = data['sentiment_polarity'].apply(convert_to_rating)

from nltk.sentiment import SentimentIntensityAnalyzer
from nltk import download

download('vader_lexicon')  # Pastikan resource sudah terunduh
sia = SentimentIntensityAnalyzer()

# Fungsi untuk VADER
def analyze_sentiment_vader(text):
    sentiment_score = sia.polarity_scores(text)
    return sentiment_score['compound']  # Gunakan compound sebagai skor

# Tambahkan kolom sentimen dengan VADER
data['sentiment_compound'] = data['keywords'].apply(analyze_sentiment_vader)

# Konversi compound ke rating
data['sentiment_rating_vader'] = data['sentiment_compound'].apply(convert_to_rating)

# Display DataFrame
data[['keywords', 'sentiment_compound', 'Rating', 'sentiment_rating_vader']]

"""# MODELING"""

data.info()

data = data.drop(columns=['Unnamed: 0', 'Clothing ID', 'Age', 'Title', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'lemmatized', 'sentiment_polarity', 'sentiment_rating'])

data = data.drop(columns=['Unnamed: 0'])

data = data.drop(columns=['clear'])

data.info()
data

data = data.dropna(subset=["Rating"])

data = data.dropna(subset=["keywords"])

data["sentiment_rating_vader"] = data["Rating"].apply(lambda x: 1 if x>= 4 else 0)
data

# data = data.drop(subset=[""])
X = data["keywords"]
y = data["sentiment_rating_vader"]

# Import the necessary function
from sklearn.model_selection import train_test_split

# Existing code remains unchanged
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

X_train =X_train.dropna()

X_train

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

X_train_tfidf

X_train_tfidf.shape

from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier from sklearn.ensemble

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_tfidf, y_train)

!pip install scikit-learn

from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier from sklearn.ensemble
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report # Import necessary metrics functions

f1_threshold = f1_score(y_test, y_pred_threshold, average='weighted') # Change 'binary' to 'weighted'

y_prob = rf.predict_proba(X_test_tfidf)
y_pred_threshold = (y_prob[:, 1] > 0.7).astype(int)

accuracy_threshold = accuracy_score(y_test, y_pred_threshold)
f1_threshold = f1_score(y_test, y_pred_threshold, average='weighted') # Change 'binary' to 'weighted'
conf_matrix_threshold = confusion_matrix(y_test, y_pred_threshold)
class_report_threshold = classification_report(y_test, y_pred_threshold)

print("Results with 0.7 probability threshold:")
print(f"Accuracy: {accuracy_threshold:.4f}")
print(f"F1 Score: {f1_threshold:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_threshold)
print("\nClassification Report:")
print(class_report_threshold)

y_pred_original = rf.predict(X_test_tfidf)
print("\nOriginal Results (0.5 threshold):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_original):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_original, average='weighted'):.4f}") # Change 'binary' to 'weighted'

import numpy as np
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam

# Assume data is already loaded: X_train, y_train, X_test, y_test

# 1. Tokenize and Prepare the Data
vocab_size = 10000  # Vocabulary size
max_length = 100  # Max length of input sequences
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')

# 2. Build the RNN Model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),  # Embedding layer
    LSTM(64, return_sequences=False),  # LSTM layer
    Dropout(0.5),  # Dropout for regularization
    Dense(32, activation='relu'),  # Fully connected layer
    Dense(1, activation='sigmoid')  # Output layer (binary classification)
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# 3. Train the Model
model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))

# 4. Evaluate the Model
y_prob = model.predict(X_test_padded)

# 5. Classify with 0.7 Threshold
y_pred_threshold = (y_prob > 0.7).astype(int)

accuracy_threshold = accuracy_score(y_test, y_pred_threshold)
f1_threshold = f1_score(y_test, y_pred_threshold)
conf_matrix_threshold = confusion_matrix(y_test, y_pred_threshold)
class_report_threshold = classification_report(y_test, y_pred_threshold)

print("Results with 0.7 probability threshold:")
print(f"Accuracy: {accuracy_threshold:.4f}")
print(f"F1 Score: {f1_threshold:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_threshold)
print("\nClassification Report:")
print(class_report_threshold)

# 6. Original 0.5 Threshold (default sigmoid output threshold)
y_pred_original = (y_prob > 0.5).astype(int)
print("\nOriginal Results (0.5 threshold):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_original):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_original):.4f}")



"""# KATA YANG SERING DIGUNAKAN"""

attributes = {
    'size': ['size', 'fit', 'small', 'large', 'tight', 'loose'],
    'quality': ['quality', 'material', 'fabric', 'stitch', 'durable', 'cheap'],
    'comfort': ['comfortable', 'uncomfortable', 'soft', 'wear', 'feel'],
    'style': ['style', 'design', 'fashion', 'look', 'beautiful', 'ugly'],
}


def extract_attributes(review, attributes):
    result = {attr: 0 for attr in attributes}
    for attr, keywords in attributes.items():
        for keyword in keywords:
            if keyword in review:
                result[attr] += 1
    return result


data['Attributes'] = data['keywords'].apply(lambda x: extract_attributes(x, attributes))


attributes_df = data['Attributes'].apply(pd.Series)
data_with_attributes = pd.concat([data, attributes_df], axis=1)


attribute_summary = attributes_df.sum()
print("Summary of Attribute Mentions:")
print(attribute_summary)

attribute_summary = attributes_df.sum()

plt.figure(figsize=(10, 6))
plt.bar(attribute_summary.index, attribute_summary.values)
plt.title("Summary of Attribute Mentions in Reviews")
plt.xlabel("Attributes")
plt.ylabel("Count of Mentions")
plt.xticks(rotation=45)
plt.show()

"""Berdasarkan data diatas terdapat 4 atribut yang paling banyak di mention oleh konsumen. diantaranya adalah size, qualitas, kenyamanan, dan juga style pakaian.


1.   Size paling banyak di mention oleh konsumen, lebih dari 18 ribu konsumen memperhatikan size pakaian yang mereka pakai dengan ukuran badan mereka.

2.   Yang kedua adalah kualitas pakaian yang digunakan, lebih dari 8 ribu pembeli memperhatikan bagaimana qualitas dari pakaian sebelum membelinya.

3. Ketiga adalah kenyamanan ketika digunakan, lebih dari 6 ribu pembeli mementingkan kenyamanan pakaian mereka

4. Dan yang keempat adalah style pakaian yang akan mereka pakai, tentu style juga penting mengingat terus adanya perubahan style mengikuti trend yang terus berjalan.

Rekomendasi bisnis:

Size
1. Sediakan berbagai variasi ukuran yang sesuai dengan berbagai bentuk tubuh (petite, regular, plus size).
2. Gunakan panduan ukuran yang jelas dan akurat pada label atau situs web, termasuk dimensi detail seperti lingkar dada, pinggang, dan panjang.
3. Tawarkan kebijakan pengembalian barang yang fleksibel, terutama untuk ukuran yang tidak sesuai.

Kualitas Produk
1. Gunakan bahan berkualitas tinggi yang tahan lama dan nyaman digunakan.
2. Lakukan kontrol kualitas yang ketat di setiap tahap produksi.
3. Transparansi material: berikan informasi tentang bahan yang digunakan.

Kenyamanan:
1. Gunakan bahan yang nyaman, seperti katun lembut, linen, atau material stretchable.
2. Desain pakaian dengan memperhatikan mobilitas dan ergonomi.

Style:
1. Pantau tren fashion melalui media sosial, runway, atau kolaborasi dengan desainer muda.
2. Luncurkan koleksi baru secara berkala untuk mengikuti tren terkini.
3. Gunakan media sosial untuk mempromosikan gaya terbaru dan menciptakan hype.
4. Hadirkan koleksi kapsul atau limited edition yang menarik perhatian konsumen.
"""

!pip install scikit-learn
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=20)  # Extract top 20 trigrams
trigram_matrix = vectorizer.fit_transform(data['keywords'])

trigram_counts = trigram_matrix.sum(axis=0).A1
trigram_features = vectorizer.get_feature_names_out()


trigram_df = pd.DataFrame({'Trigram': trigram_features, 'Count': trigram_counts}).sort_values(by='Count', ascending=False)


plt.figure(figsize=(12, 6))
plt.barh(trigram_df['Trigram'], trigram_df['Count'], color='skyblue')
plt.gca().invert_yaxis()
plt.title("Top 20 Most Frequent Trigrams in Reviews")
plt.xlabel("Count")
plt.ylabel("Trigram")
plt.show()

!pip install scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(data['keywords'])

num_topics = 5
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(tfidf_matrix)

topics = {}
num_top_words = 10
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

for idx, topic in enumerate(lda.components_):
    top_words = [tfidf_feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]
    topics[f"Topic {idx+1}"] = top_words

topics

topic_words = [', '.join(words) for words in topics.values()]
topic_counts = [len(words) for words in topics.values()]


plt.figure(figsize=(12, 8))
plt.barh(list(topics.keys()), topic_counts, color='skyblue')
plt.title("Top Words for Each Topic Identified by LDA")
plt.xlabel("Number of Top Words")
plt.ylabel("Topics")


for i, words in enumerate(topic_words):
    plt.text(topic_counts[i] + 0.1, i, words, va='center', fontsize=10, color='black')

plt.show()

"""Berdasarkan grafik diatas, bisa dikatakan LDA berhasil mengelompokkan berdasarkan tema distingtif pada setiap topik. Namun masih terdapat kesamaan dalam setiap topik, seperti size, fabric, and comfortable yang masih muncul di beberapa topik. Hal ini menunjukkan bahwa model belum sepenuhnya memisahkan topik dengan baik.

Dari grafik diatas:
*   Topik 1 & 2 konsumen sangat peduli tentang ukuran dan kecocokan pakaian. Bisnis perlu menyediakan ukuran yang bervariasi, panduan ukuran yang jelas, serta menyoroti fitur "fit" dalam pemasaran.
*   Topik 3 fokus pada bahan yang lembut, nyaman, dan fleksibel untuk berbagai aktivitas. Kenyamanan menjadi salah satu faktor utama yang mendorong kepuasan konsumen.
*   Topik 4 & 5 konsumen memperhatikan gaya dan kualitas bahan. Menyediakan pakaian dengan desain modern dan bahan berkualitas tinggi dapat meningkatkan daya tarik produk.

Sehingga, hal yang perlu di pertimbangkan dalam bisnis yaitu:
1. Peningkatan produk dengan cara fokus pada bahan yang nyaman, ukuran yang sesuai, dan desain yang trendi.
2. Melakukan pemasaran tersegmentasi dengan cara menyoroti manfaat kenyamanan, kualitas bahan, dan fleksibilitas ukuran dalam iklan atau promosi.
3. Mengumpulkan umpan balik untuk memperbaiki area seperti panduan ukuran atau desain produk.


"""

